{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samuel91-design/DeepLearningPython/blob/master/QAML_Inference_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypeXsY_pjtGI"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "!pip install -q transformers accelerate einops langchain bitsandbytes\n",
        "\n",
        "# update or install the necessary libraries\n",
        "!pip install -q --upgrade openai\n",
        "!pip install -q --upgrade langchain\n",
        "!pip install -q --upgrade python-dotenv\n",
        "\n",
        "# For API\n",
        "!pip -q install fastapi\n",
        "!pip -q install pyngrok\n",
        "!pip -q install uvicorn\n",
        "!pip -q install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ETejngFZ4jN",
        "outputId": "dbd8b406-23b2-4237-c92b-4b747ffe9b65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYfeJgD_j96r"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "pipeline = pipeline(\n",
        "    'text-generation',\n",
        "    model=model_name,\n",
        "    tokenizer=tokenizer,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    max_length=512,\n",
        "    do_sample=False,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPgmlRlQVZZx"
      },
      "outputs": [],
      "source": [
        "# Falcon\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "falcon = HuggingFacePipeline(pipeline=pipeline, model_kwargs={\"temperature\": 1.0})\n",
        "\n",
        "def live_qa_falcon_answer(riddle):\n",
        "  template = \"\"\"\n",
        "      You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must provide a short answer to a riddle along with value between 0 and 1 inicating how sure you are of your answer.\n",
        "      You must present your answer in a json format with the keys 'answer' to hold your answer and 'confidence' to hold your estimated confidence value.\n",
        "      Higher confidence values mean you're sure of your answer, whereas lower confidence values mean you are not sure of your answer.\n",
        "      Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "      Output: <json object> 'answer': 'oscillator', 'confidence':0.8\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "      Riddle: {riddle}\n",
        "\n",
        "      Output: 'answer': '', 'confidence': ''\n",
        "  \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"riddle\"])\n",
        "\n",
        "  falcon_chain = LLMChain(prompt=prompt, llm=falcon)\n",
        "  answer = falcon_chain.run(riddle)\n",
        "  answer = answer.replace('{', '').replace('}', '').replace('\\n', '')\n",
        "  answer = '{' + answer + '}'\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_qa_falcon_answer(riddle_content):\n",
        "  template = \"\"\"\n",
        "      You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must provide a short answer to a riddle. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "\n",
        "      Riddle: {riddle}\n",
        "\n",
        "      Answer:\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"riddle\"])\n",
        "  falcon_chain = LLMChain(prompt=prompt, llm=Falcon)\n",
        "  answer = falcon_chain.run({\"riddle\":riddle})\n",
        "  return answer"
      ],
      "metadata": {
        "id": "GDROUQ9JvdQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01WYjWyqZSLP"
      },
      "outputs": [],
      "source": [
        "#os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] =\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TD9PmLbZYcx"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# chat mode instance\n",
        "chat = ChatOpenAI(temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz2M57IhZiy6"
      },
      "outputs": [],
      "source": [
        "# ChatGPT\n",
        "def live_qa_chatgpt_answer(riddle):\n",
        "  template = \"\"\"\n",
        "      You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must provide a short answer to a riddle along with value between 0 and 1 inicating how sure you are of your answer.\n",
        "      You must present your answer in a json format with the keys 'answer' to hold your answer and 'confidence' to hold your estimated confidence value.\n",
        "      Higher confidence values mean you're sure of your answer, whereas lower confidence values mean you are not sure of your answer.\n",
        "      Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "      Output: <json object> 'answer': 'oscillator', 'confidence':0.8\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "      Riddle: {riddle}\n",
        "\n",
        "      Output: 'answer': '', 'confidence': ''\n",
        "  \"\"\"\n",
        "\n",
        "  answer = chat([HumanMessage(content=template.format(riddle=riddle))])\n",
        "  answer = answer.content.replace('{', '').replace('}', '')\n",
        "  return '{' + answer +'}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_qa_chatgpt_answer(riddle_content):\n",
        "  template = \"\"\"\n",
        "      You are a science prodigy currently competing in a National Science competition. You are now in the fifth round, where you must provide a short answer to a riddle. Remember, your answer should consist of just the term the riddle is pointing to, and nothing else. Adding additional text will result in point deductions.\n",
        "      Here's an example to guide you:\n",
        "      Riddle: you might think i am a rather unstable character because i never stay at one place, however my motion obeys strict rules and i always return to where i started and even if i have to leave that spot again i do it in strict accordance to time, i can be named in electrical and mechanical contexts in all cases i obey the same mathematical rules, in order to fully analyse me you would think about a stiffness or force constant restoring force and angular frequency,\n",
        "      Answer: oscillator\n",
        "\n",
        "      Read the riddle below and provide the correct answer.\n",
        "      Riddle: {riddle}\n",
        "\n",
        "      Answer:\"\"\"\n",
        "\n",
        "  answer = chat([HumanMessage(content=template.format(riddle=riddle_content))])\n",
        "  return answer.content"
      ],
      "metadata": {
        "id": "WCh7Hj_vvU_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIKbvvlj_V59"
      },
      "outputs": [],
      "source": [
        "import uvicorn\n",
        "import fastapi\n",
        "from pyngrok import ngrok\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOYahAR5AdHn"
      },
      "outputs": [],
      "source": [
        "class DemoInputText(BaseModel):\n",
        "  text: str\n",
        "\n",
        "class LiveInputText(BaseModel):\n",
        "  clues: str\n",
        "  is_start_of_riddle: bool = False\n",
        "  is_end_of_riddle: bool = False\n",
        "  clue_count: int = 0\n",
        "\n",
        "class OutputText(BaseModel):\n",
        "  falcon: str\n",
        "  chatGPT: str = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc_WVp0ULb_n",
        "outputId": "b2436690-28ce-4952-9b71-1f2eea7e6c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2Rn9eOsY1tAIYyaIyMRZ9qnm16S_3D1m5KUKwXMhXR79u1h3u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLBfbRR1Ap8A"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "def filter_answers(answers, confidence_threshold):\n",
        "    confidence = answers['confidence']\n",
        "    if float(confidence) >= confidence_threshold:\n",
        "      return answers['answer']\n",
        "\n",
        "    return ''\n",
        "\n",
        "\n",
        "app = fastapi.FastAPI()\n",
        "\n",
        "\n",
        "@app.get(\"/live_qa\", response_model=OutputText)\n",
        "def live_answer(input_data: LiveInputText):\n",
        "    print(\"riddle:\", input_data.clues)\n",
        "    ct = 0.5  # Hardcoded confidence threshold\n",
        "    print(\"confidence threshold:\", ct)\n",
        "    falcon_output = live_qa_falcon_answer(riddle=input_data.clues)\n",
        "    try:\n",
        "        falcon_output = ast.literal_eval(falcon_output)\n",
        "    except (ValueError, SyntaxError):\n",
        "        falcon_output = {\"answer\": '', \"confidence\": 0.0}\n",
        "    chatGPT_output = live_qa_chatgpt_answer(input_data.clues)\n",
        "    try:\n",
        "      chatGPT_output = ast.literal_eval(chatGPT_output)\n",
        "    except(ValueError, SyntaxError):\n",
        "      chatGPT_output = {\"answer\":'', \"confidence\": 0.0}\n",
        "    print(\"falcon:\", falcon_output)\n",
        "    print(\"chatGPT:\", chatGPT_output)\n",
        "\n",
        "    answers = {\n",
        "        \"falcon\": filter_answers(falcon_output, ct),  # ct represents the confidence threshold.\n",
        "        \"chatGPT\": filter_answers(chatGPT_output, ct)\n",
        "    }\n",
        "\n",
        "    return answers\n",
        "\n",
        "@app.get('/demo_qa', response_model=OutputText)\n",
        "def demo_answer(input_data: DemoInputText):\n",
        "    riddle_content = input_data.text\n",
        "    falcon_ans = demo_qa_falcon_answer(riddle_content)\n",
        "\n",
        "    print(\"falcon:\", falcon_output)\n",
        "\n",
        "    answers = {\n",
        "        \"falcon\": falcon_ans\n",
        "    }\n",
        "    return answers\n",
        "\n",
        "\n",
        "@app.get(\"/qa-test\", response_model=OutputText)\n",
        "async def qa_test():\n",
        "    return {\n",
        "        \"falcon\": \"Falcon Says Hello!\",\n",
        "        \"chatGPT\": \"ChatGPT Says Hello!\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2Rn9eOsY1tAIYyaIyMRZ9qnm16S_3D1m5KUKwXMhXR79u1h3u"
      ],
      "metadata": {
        "id": "2mJ8YGnqbevb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "id": "FMlBe-Po07ug"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}